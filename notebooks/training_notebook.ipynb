{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transform\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "\n",
    "# import src.model.trainer as trainer\n",
    "# import src.model.config as config\n",
    "# import src.utils.other_utils as other_utils\n",
    "# import src.data.make_dataset as data_funcs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=config.DEVICE)\n",
    "    # model.load_state_dict(checkpoint)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Trainer():\n",
    "\n",
    "  def __init__(self, model, criterion = None, optimizer = None, device = \"cpu\"):\n",
    "    \n",
    "    self.model = model\n",
    "    self.criterion = criterion\n",
    "    self.optimizer = optimizer\n",
    "    self.device = device\n",
    "    \n",
    "\n",
    "  def train_step(self, train_dataloader):\n",
    "    \n",
    "    # Set model to training mode\n",
    "    self.model.train()\n",
    "\n",
    "    epoch_loss, epoch_accuracy = 0, 0\n",
    "\n",
    "    for batch, (X,y) in enumerate(train_dataloader):\n",
    "      X = X.to(self.device)\n",
    "      y = y.to(self.device)\n",
    "\n",
    "      # Reset Gradients\n",
    "      self.optimizer.zero_grad()\n",
    "\n",
    "      # Prediction\n",
    "      out = self.model(X)\n",
    "\n",
    "      # Calculation Loss\n",
    "      loss = self.criterion(out, y)\n",
    "\n",
    "      # Calculating Gradients\n",
    "      loss.backward()\n",
    "\n",
    "      # Update Weights\n",
    "      self.optimizer.step()\n",
    "\n",
    "      # Calculating Performance Metrics\n",
    "      epoch_loss += loss.detach().item() / X.shape[0]\n",
    "      #epoch_accuracy += (torch.argmax(torch.softmax(out[:,0,:], dim=1), dim=1) == y).sum() / X.shape[0]\n",
    "\n",
    "    return epoch_loss#, epoch_accuracy\n",
    "\n",
    "\n",
    "  def eval_step(self, val_dataloader):\n",
    "    \n",
    "    # Set model to training mode\n",
    "    self.model.eval()\n",
    "\n",
    "    epoch_loss, epoch_accuracy = 0, 0\n",
    "    y_trues, y_probs = [], []\n",
    "    #epoch_accuracy = 0\n",
    "    with torch.inference_mode():\n",
    "      for batch, (X,y) in enumerate(val_dataloader):\n",
    "        X = X.to(self.device)\n",
    "        y = y.to(self.device)\n",
    "\n",
    "        # Prediction\n",
    "        out = self.model(X)\n",
    "        # Calculation Loss\n",
    "        loss = self.criterion(out, y)\n",
    "\n",
    "        # Calculating Performance Metrics\n",
    "        epoch_loss += loss.item() / X.shape[0]\n",
    "        #epoch_accuracy += (torch.argmax(torch.softmax(out[:,0,:], dim=1), dim=1) == y).sum() / X.shape[0]\n",
    "\n",
    "    return epoch_loss#, epoch_accuracy\n",
    "\n",
    "  def predict_step(self, val_dataloader):\n",
    "\n",
    "    self.model.eval()\n",
    "\n",
    "    y_preds = []\n",
    "    y_probs = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "      for batch, (X,y) in enumerate(val_dataloader):\n",
    "        X = X.to(self.device)\n",
    "        y = y.to(self.device)\n",
    "\n",
    "        # Prediction\n",
    "        out = self.model(X)\n",
    "\n",
    "        # Calculating Performance Metrics\n",
    "        y_prob = torch.softmax(out[:,0,:], dim=1)\n",
    "        y_pred = torch.argmax(torch.softmax(out[:,0,:], dim=1), dim=1)\n",
    "\n",
    "        y_probs = [y_prob[index, y_pred[index]].item() for index in range(len(y_pred))]\n",
    "        y_preds.extend(y_pred)\n",
    "\n",
    "    return y_probs, y_preds\n",
    "\n",
    "\n",
    "  def train(self, train_dataloader, val_dataloader, num_epochs = 5):\n",
    "    xx = False\n",
    "    if xx:\n",
    "        load_checkpoint(\n",
    "            \"/Users/mpekey/Desktop/Mert_SabanciUniv/CS518/HoneyPollenClassification/model_checkpoints/model_ch.pth\",\n",
    "            self.model,\n",
    "            self.optimizer,\n",
    "            1e-4,\n",
    "        )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "      train_loss, train_accuracy = self.train_step(train_dataloader)\n",
    "      val_loss, val_accuracy = self.eval_step(val_dataloader)\n",
    "\n",
    "      if xx:\n",
    "        save_checkpoint(self.model, self.optimizer, filename=\"/Users/mpekey/Desktop/Mert_SabanciUniv/CS518/HoneyPollenClassification/model_checkpoints/model_ch.pth\")\n",
    "\n",
    "      # Logging\n",
    "      print(\n",
    "          f\"Epoch: {epoch+1} | \"\n",
    "          f\"train_loss: {train_loss:.5f}, \"\n",
    "          f\"train_acc: {train_accuracy:.5f}, \"\n",
    "          f\"val_loss: {val_loss:.5f}, \"\n",
    "          f\"val_acc: {val_accuracy:.5f}\"\n",
    "      )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "def get_pollen_family(pollen_type):\n",
    "\n",
    "    if pollen_type in ['Acanthus dioscoridis', 'Acanthus sp']:\n",
    "        return 'Acanthaceae'\n",
    "    elif pollen_type in ['Sambucus ebulus', 'Sambucus nigra', 'Viburnum lantana','Viburnum lanata']:\n",
    "        return 'Adoxaceae'\n",
    "    elif pollen_type in ['Chenopodium foliosum','Amaranthaceae sp']:\n",
    "        return 'Amaranthaceae'\n",
    "    elif pollen_type in ['Allium rotundum', 'Allium ampeloprasum', 'Allium sp']:\n",
    "        return 'Amaryllidaceae'\n",
    "    elif pollen_type in ['Cotinus coggygria', 'Rhus coriaria', 'Rhus sp']:\n",
    "        return 'Anacardiaceae'\n",
    "    elif pollen_type in ['Eryngium campestre', 'Ferula orientalis', 'Malabaila lasiocarpa', 'Lecokia cretica', 'Pimpinella sp','Apiaceae sp']:\n",
    "        return 'Apiaceae'\n",
    "    elif pollen_type in ['Leopoldia tenuiflora','Ornithogalum narbonense','Muscari tenuifolium']:\n",
    "        return 'Asparagaceae'\n",
    "    elif pollen_type in ['Gundelia sp','Circium arvense','Centaurea urvellei','Cardus','Matricaria chamomila','Scorzonera latifolia','Asteraceae sp','Artemisia absinthium','Achillea arabica', 'Achilla arabica', 'Achillea millefolium', 'Achillea vermicularis','Anthemis cretica','Arctium minus','Arctium minus','Bellis perennis',\n",
    "    'Carduus nutans','Centaurea bingoelensis','Centaurea iberica','Centaurea kurdica','Centaurea saligna','Centaurea solstitialis','Centaurea spectabilis',\n",
    "    'Centaurea urvillei','Centaurea virgata','Chondrilla brevirostris','Chondrilla juncea','Cichorium intybus','Cirsium arvense','Cirsium yildizianum',\n",
    "    'Cota altissima','Crepis sancta','Cyanus triumfettii','Echinops pungens','Gundelia tournefortii','Helichrysum arenarium','Helichrysum plicatum',\n",
    "    'Iranecio eriospermus','Senecio eriospermus','Matricaria chamomilla','Onopordum acanthium','Onopordum acanthium','Tanacetum balsamita','Tanacetum zahlbruckneri',\n",
    "    'Taraxacum campylodes','Taraxacum officinale','Tussilago farfara','Xeranthemum annuum','Xeranthemum longipapposum','Artemisia sp','Carduus sp','Centaurea sp',\n",
    "    'Cichorium sp','Cirsium sp','Cirsium yildizianum','Echinops sp','Echinops sp','Helianthus sp','Helichrysum sp','Onopordum sp','Ptilostemon sp','Xanthium sp','Xeranthemum sp','Xeranthemum longipopposum','Xeranthemum annum']:\n",
    "        return 'Asteraceae'\n",
    "    elif pollen_type in ['Alkanna orientalis','Anchusa azurea','Anchusa leptophylla','Cerinthe minor','Echium italicum','Myosotis alpestris','Myosotis laxa','Myosotis stricta','Myosotis sylvatica','cyanea','Phyllocara aucheri','Anchusa sp','Echium sp']:\n",
    "        return 'Boraginaceae'\n",
    "    elif pollen_type in ['Capsella bursa pastoris','Aethionema grandiflorum', 'Capsella bursa-pastoris', 'Isatis glauca','Lepidium draba','Raphanus raphanistrum']:\n",
    "        return 'Brassicaceae'\n",
    "    elif pollen_type in ['Campanula glomerata','Campanula involucrata', 'Campanula propinqua','Campanula stricta', 'stricta', 'Campanula sp']:\n",
    "        return 'Campanulaceae'\n",
    "    elif pollen_type in ['Centranthus longifolius','Centranthus longiflorus', 'Morina persica', 'Scabiosa columbaria', 'Scabiosa rotata', 'Cephalaria sp', 'Scabiosa sp', 'Valeriana sp']:\n",
    "        return 'Caprifoliaceae'\n",
    "    elif pollen_type in ['Cerastium armeniacum', 'Saponaria prostrata', 'Saponaria viscosa', 'Saponaria viscosa', 'Silene spergulifolia','Dianthus sp','Silene sp','Silene compacta']:\n",
    "        return 'Caryophyllaceae'\n",
    "    elif pollen_type in ['Chenopodium sp']:\n",
    "        return 'Chenopodiaceae'\n",
    "    elif pollen_type in ['Convolvulus arvensis', 'Convolvulus galaticus', 'Convolvulus lineatus', 'Convolvulus sp']:\n",
    "        return 'Convolvulaceae'\n",
    "    elif pollen_type in ['Cornus sanguinea']:\n",
    "        return 'Cornaceae'\n",
    "    elif pollen_type in ['Phedimus obtusifolius']:\n",
    "        return 'Crassulaceae'\n",
    "    elif pollen_type in ['Elaeagnus angustifolia']:\n",
    "        return 'Elaeagnaceae'\n",
    "    elif pollen_type in ['Euphorbia esula','tommasiniana', 'Euphorbia macrocarpa', 'Euphorbia sp']:\n",
    "        return 'Euphorbiaceae'\n",
    "    elif pollen_type in ['Glycyrrhiza glabra','Astragalus topolanense','Astragalus sp','Astragalus pinetorium','Astragalus lagopoides','Securigera varia','Astracantha gummifera','Trifolium campestre-yeniden','Astragalus gummifer','Astracantha kurdica','Astragalus kurdicus','Astracantha muschiana','Astragalus muschianus','Astragalus aduncus','Astragalus bingollensis','Astragalus bustillosii','Astragalus brachycalyx','brachycalyx','Astragalus caspicus','Astrgalus lagopoides','Astrgalus lagopoides Lam','Astragalus onobrychis','Astragalus oocephalus','Astragalus pinetorum','Astragalus saganlugensis','Astragalus topalanense','Colutea cilicica','Genista aucheri','Genista aucheri','Lathyrus brachypterus','Lathyrus satdaghensis','Lotus corniculatus','Lotus gebelia','Hedysarum varium','Medicago sativa','Melilotus albus','Melilotus officinalis','Onobrychis viciifolia','Ononis spinosa','Robinia pseudoacacia','Robinia pseudoacacia','Trifolium campestre','Trifolium diffusum','Trifolium nigrescens','Trifolium pauciflorum','Trifolium pratense','Trifolium resupinatum','Vicia cracca','cracca','Astragalus gummifer','Astragalus longifolius','Astragalus topalanense','Astragalus spp','Coronilla sp','Hedysarum sp','Lathyrus sp','Lotus sp','Melilotus sp','Trifolium spp','Vicia sp']:\n",
    "        return 'Fabaceae'\n",
    "    elif pollen_type in ['Quercus petraea', 'pinnatiloba']:\n",
    "        return 'Fagaceae'\n",
    "    elif pollen_type in ['Geranium tuberosum', 'Geranium sp']:\n",
    "        return 'Geraniaceae'\n",
    "    elif pollen_type in ['Hypericum sp','Hypericum lydium', 'Hypericum perforatum','Hypericum scabrum','Hypericum spp']:\n",
    "        return 'Hypericaceae'\n",
    "    elif pollen_type in ['Ixiolirion tataricum']:\n",
    "        return 'Ixioliriaceae'\n",
    "    elif pollen_type in ['Marrubium astracanium','Salvia palestina','Ajuga chamaepitys','chia','Lamium album','Lamium garganicum','Lamium macrodon','Marrubium astracanicum','Marrubium vulgare','Mentha longifolia','longifolia','Mentha spicata','Nepeta baytopii','Nepeta cataria','Nepeta nuda','Nepeta trachonitica','Origanum acutidens','Origanum vulgare','gracile','Phlomis armeniaca','Phlomis herba-venti','pungens','Phlomis pungens','Phlomis kurdica','Salvia frigida','Salvia limbata','Salvia macrochlamys','Salvia multicaulis','Salvia palaestina','Salvia sclarea','Salvia staminea','Salvia trichoclada','Salvia verticillata','Salvia virgata','Satureja hortensis','Stachys annua','Stachys lavandulifolia','Teucrium chamaedrys','Teucrium orientale','Teucrium orientale','Teucrium polium','Thymus kotschyanus','Thymus pubescens','Lamium macrodon','Lamium sp','Origanum sp','Phlomis sp','Teucrium sp','Thymus spp']:\n",
    "        return 'Lamiaceae'\n",
    "    elif pollen_type in ['Linum mucronatum','armenum']:\n",
    "        return 'Linaceae'\n",
    "    elif pollen_type in ['Lythrum salicaria']:\n",
    "        return 'Lythraceae'\n",
    "    elif pollen_type in ['Malvaceae','Alcea apterocarpa','Alcea remotiflora','Malva neglecta','Alcea sp','Malva sp','Tilia sp']:\n",
    "        return 'Malvaceae'\n",
    "    elif pollen_type in ['Morus alba']:\n",
    "        return 'Moraceae'\n",
    "    elif pollen_type in ['Epilobium parviflorum']:\n",
    "        return 'Onagraceae'\n",
    "    elif pollen_type in ['Fumaria parviflora','Fumaria schleicheri','microcarpa','Papaver dubium','Papaver orientale']:\n",
    "        return 'Papaveraceae'\n",
    "    elif pollen_type in ['Anarrhinum orientale','Anarhinum orientale','Globularia trichosantha','Lagotis stolonifera','Linaria pyramidata','Plantago lanceolata','Plantago major','Plantago media','Linaria sp']:\n",
    "        return 'Plantaginaceae'\n",
    "    elif pollen_type in ['Acantholimon acerosum','Acantholimon armenum','Acantholimon calvertii','Acantholimon sp']:\n",
    "        return 'Plumbaginaceae'\n",
    "    elif pollen_type in ['Poaceae','Zea mays']:\n",
    "        return 'Poaceae'\n",
    "    elif pollen_type in ['Polygonum cognatum','Rheum ribes','Rumex acetosella','Rumex scutatus','Rumex sp']:\n",
    "        return 'Polygonaceae'\n",
    "    elif pollen_type in ['Lysimachia punctata', 'Lysimacha vulgaris']:\n",
    "        return 'Primulaceae'\n",
    "    elif pollen_type in ['Portulaca sp']:\n",
    "        return 'Portulacaceae'\n",
    "    elif pollen_type in ['Ranunculus kotchii','Ficaria fascicularis','Ranunculus kochii','Ranunculus heterorrhizus','Ranunculus kotschyi','Ranunculus sp']:\n",
    "        return 'Ranunculaceae'\n",
    "    elif pollen_type in ['Paliurus spina-christi']:\n",
    "        return 'Rhamnaceae'\n",
    "    elif pollen_type in ['Crateagus orientalis','Crateagus monogyna','Potentilla inclinata','Rosaceae','Sanguisorba minÃ¶r','Agrimonia repens','Cotoneaster nummularius','Crataegus orientalis','Crataegus monogyna','Filipendula ulmaria','Malus sylvestris','Potentilla anatolica','Potentilla argentea','Prunus divaricata','ursina','Pyrus elaeagnifolia','Rosa canina','Rosa foetida','Rubus caesius','Rubus sanctus','Sanguisorba minor','lasiocarpa','Sorbus torminalis','Filipendula sp','Potentilla sp','Rosa canina']:\n",
    "        return 'Rosaceae'\n",
    "    elif pollen_type in ['Galium consanguineum','Galium verum','Galium sp']:\n",
    "        return 'Rubiaceae'\n",
    "    elif pollen_type in ['Citrus sp']:\n",
    "        return 'Rutaceae'\n",
    "    elif pollen_type in ['Salix alba','Salix caprea','Salix sp']:\n",
    "        return 'Salicaceae'\n",
    "    elif pollen_type in ['Verbascum armenum','Verbascum diversifolium','Verbascum gimgimense','Verbascum lasianthum','Verbascum sinuatum','Verbascum spp','Verbascum sinatum']:\n",
    "        return 'Scrophulariaceae'\n",
    "    elif pollen_type in ['Tamarix smyrnensis','Tamarix tetrandra']:\n",
    "        return 'Tamaricaceae'\n",
    "    elif pollen_type in ['Eremurus spectabilis','Eremurus sp']:\n",
    "        return 'Xanthorrhoeaceae'\n",
    "    elif pollen_type in ['Tribulus terrestris']:\n",
    "        return 'Zygophyllaceae'\n",
    "    else:\n",
    "        return 'notfound'\n",
    "\n",
    "def get_dataset_roots(dir_path):\n",
    "\n",
    "    class_dict = {}\n",
    "    \n",
    "    for dirpath, dirnames, filenames in os.walk(dir_path):\n",
    "        class_name = dirpath.split('/')[-1]\n",
    "        if class_name != 'dataset':\n",
    "            for filename in os.listdir(dirpath):\n",
    "                class_dict[os.path.join(dir_path, class_name, filename)] = class_name\n",
    "                \n",
    "    data_dict = pd.DataFrame({'img_file':list(class_dict.keys()), 'type':list(class_dict.values())})\n",
    "    data_dict['family'] = data_dict['type'].apply(get_pollen_family)\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def split_datasets(pollen_df, class_amt_df, label_name = 'type'):\n",
    "\n",
    "    class_names = pollen_df[label_name].unique()\n",
    "\n",
    "    first_check = True\n",
    "\n",
    "    for t in class_names:\n",
    "\n",
    "        img_amount = class_amt_df[class_amt_df[label_name] == t].img_num.iloc[0]\n",
    "\n",
    "        if label_name == 'type':\n",
    "            if img_amount >= 5:\n",
    "                split_ratio = (0.6,0.2,0.2) # May be changed\n",
    "            elif img_amount == 4:\n",
    "                split_ratio = (0.5,0.25,0.25)\n",
    "            elif img_amount == 3:\n",
    "                split_ratio = (0.33,0.33,0.33)\n",
    "            elif img_amount == 2:\n",
    "                split_ratio = (0.5,0.5,0.0)\n",
    "        elif label_name == 'family':\n",
    "            if img_amount >= 5:\n",
    "                split_ratio = (0.6,0.2,0.2)\n",
    "            elif img_amount == 4:\n",
    "                split_ratio = (0.5,0.25,0.25)\n",
    "            elif img_amount == 3:\n",
    "                split_ratio = (0.33,0.33,0.33)\n",
    "            elif img_amount == 2:\n",
    "                split_ratio = (0.5,0.5,0.0)\n",
    "\n",
    "        idx_list = list(pollen_df[pollen_df[label_name] == t].index)\n",
    "\n",
    "        train_idx = np.random.choice(idx_list, size=math.ceil(len(idx_list)*split_ratio[0]), replace=False)\n",
    "        remaining_idx = []\n",
    "        for i in idx_list:\n",
    "            if i not in train_idx:\n",
    "                remaining_idx.append(i)\n",
    "\n",
    "        val_idx = np.random.choice(remaining_idx, size=math.ceil(len(remaining_idx)*(split_ratio[1]/(1-split_ratio[0]))), replace=False)\n",
    "\n",
    "        test_idx = []\n",
    "        for i in remaining_idx:\n",
    "            if i not in val_idx:\n",
    "                test_idx.append(i)\n",
    "\n",
    "        if first_check:\n",
    "            train_data = pollen_df.loc[train_idx,['img_file', label_name]]\n",
    "            val_data = pollen_df.loc[val_idx,['img_file', label_name]]\n",
    "            test_data = pollen_df.loc[test_idx,['img_file', label_name]]\n",
    "            first_check = False\n",
    "        else:\n",
    "            train_data = pd.concat([train_data, pollen_df.loc[train_idx,['img_file', label_name]]], axis = 0)\n",
    "            val_data = pd.concat([val_data, pollen_df.loc[val_idx,['img_file', label_name]]], axis = 0)\n",
    "            test_data = pd.concat([test_data, pollen_df.loc[test_idx,['img_file', label_name]]], axis = 0)\n",
    "\n",
    "    return train_data.reset_index(drop=True), val_data.reset_index(drop=True), test_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "class PollenDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, transform=None, is_family=False):\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "        self.data = data\n",
    "        if is_family:\n",
    "            self.class_names = self.data['family'].unique()\n",
    "        else:\n",
    "            self.class_names = self.data['type'].unique()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_root = self.data.iloc[idx,0]\n",
    "        label = self.data.iloc[idx, 1]\n",
    "        \n",
    "        #img = torchvision.io.read_image(img_root)\n",
    "        img = Image.open(img_root)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        print(img.shape)\n",
    "        print(label)\n",
    "        return (img, label)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norm Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "def get_data_transform(means, stdevs, is_train = True):\n",
    "\n",
    "    if is_train:\n",
    "        data_transform = T.Compose([\n",
    "            T.Resize((32,32)),\n",
    "            T.Normalize(mean = means, std = stdevs),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "    else:\n",
    "        data_transform = T.Compose([\n",
    "            T.Resize((32,32)),\n",
    "            T.Normalize(mean = means, std = stdevs),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "    return data_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalization_params(dataset): ## Tum dataset olarak degistir\n",
    "\n",
    "    X, _ = dataset\n",
    "    means = []\n",
    "    stdevs = []\n",
    "    \n",
    "    n_channels = X.shape[1]\n",
    "    for c in range(n_channels):\n",
    "        mean = torch.mean(X[:, c])\n",
    "        std = torch.std(X[:, c])\n",
    "\n",
    "        means.append(mean)\n",
    "        stdevs.append(std)\n",
    "\n",
    "    return means, stdevs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n",
      "Brassicaceae\n",
      "torch.Size([3, 32, 32])\n",
      "Brassicaceae\n",
      "torch.Size([3, 32, 32])\n",
      "Brassicaceae\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/j1/6_drkzkx1qb41g_fvzqvnz340000gn/T/ipykernel_12570/3257878776.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mdummy_train_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPollenDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDUMMY_TRANSFORM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_family\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mclass_type\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'family'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdevs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_normalization_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_train_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mtrain_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdevs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/j1/6_drkzkx1qb41g_fvzqvnz340000gn/T/ipykernel_12570/1360858616.py\u001b[0m in \u001b[0;36mget_normalization_params\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_normalization_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m## Tum dataset olarak degistir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstdevs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "class_type = 'family'\n",
    "DUMMY_TRANSFORM = T.Compose([\n",
    "            T.Resize((32,32)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean = (0,0,0), std = (1,1,1))\n",
    "        ])\n",
    "\n",
    "DATA_ROOT = '/Users/mpekey/Desktop/Mert_SabanciUniv/CS518/HoneyPollenClassification/dataset'\n",
    "\n",
    "pollen_df = get_dataset_roots(DATA_ROOT)\n",
    "family_amt_df = pollen_df['family'].value_counts().reset_index().rename(columns={'index':'family', 'family':'img_num'})\n",
    "type_amt_df = pollen_df['type'].value_counts().reset_index().rename(columns={'index':'type', 'type':'img_num'})\n",
    "\n",
    "train_df, val_df, test_df = split_datasets(pollen_df, family_amt_df, label_name = class_type)\n",
    "\n",
    "dummy_train_dataset = PollenDataset(data=train_df, transform=DUMMY_TRANSFORM, is_family=True if class_type=='family' else False)\n",
    "means, stdevs = get_normalization_params(dummy_train_dataset)\n",
    "\n",
    "train_transform = get_data_transform(means, stdevs, is_train=True)\n",
    "test_transform = get_data_transform(means, stdevs, is_train=False)\n",
    "\n",
    "# Create Datasets\n",
    "#Dataset() icine koy\n",
    "train_dataset = PollenDataset(data=train_df, transform=train_transform, is_family=True if class_type=='family' else False)\n",
    "val_dataset = PollenDataset(data=val_df, transform=test_transform, is_family=True if class_type=='family' else False)\n",
    "test_dataset = PollenDataset(data=test_df, transform=test_transform, is_family=True if class_type=='family' else False)\n",
    "\n",
    "# Create Dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Model\n",
    "from torchvision.models import resnet50#, ResNet50_Weights\n",
    "model = resnet50()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "model_trainer = Trainer(model = model,\n",
    "                                criterion = criterion,\n",
    "                                optimizer = optimizer, \n",
    "                                device = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.train(train_dataloader, val_dataloader, num_epochs = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
